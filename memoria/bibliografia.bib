%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%    Papers
%

@inproceedings{masina2020accesibility,
  author   = {Masina, Fabio and Orso, Valeria and Pluchino, Patrik and Dainese, Giulia and Volpato, Stefania and Nelini, Cristian and Mapelli, Daniela and Spagnolli, Anna and Gamberini, Luciano},
  journal  = {J Med Internet Res},
  title    = {Investigating the Accessibility of Voice Assistants With Impaired Users: Mixed Methods Study},
  year     = {2020},
  issn     = {1438-8871},
  month    = {10},
  number   = {9},
  pages    = {e18431},
  volume   = {22},
  doi      = {10.2196/18431},
  keywords = {voice assistants; accessibility; cognitive functions; disability; ambient assisted living},
  url      = {http://www.jmir.org/2020/9/e18431/},
}

@inproceedings{graves2013blstm,
author = {Graves, Alex and Jaitly, Navdeep and Mohamed, Abdel-rahman},
year = {2013},
month = {12},
pages = {273-278},
title = {Hybrid speech recognition with Deep Bidirectional LSTM},
journal = {2013 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2013 - Proceedings},
doi = {10.1109/ASRU.2013.6707742}
}

@inproceedings{xiao2016blstm_sentiments,
author="Xiao, Zheng and Liang, PiJun",
editor="Sun, Xingming and Liu, Alex and Chao, Han-Chieh and Bertino, Elisa",
title="Chinese Sentiment Analysis Using Bidirectional LSTM with Word Embedding",
booktitle="Cloud Computing and Security",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="601--610",
isbn="978-3-319-48674-1"
}

@inproceedings{vaswani2017transformers,
  doi = {10.48550/ARXIV.1706.03762},
  url = {https://arxiv.org/abs/1706.03762},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Attention Is All You Need},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{wang2020transformers_asr,
	doi = {10.1109/icassp40776.2020.9054345},
	url = {https://doi.org/10.1109%2Ficassp40776.2020.9054345},
	year = 2020,
	month = {5},
	publisher = {IEEE},
	author = {Yongqiang Wang and Abdelrahman Mohamed and Due Le and Chunxi Liu and Alex Xiao and Jay Mahadeokar and Hongzhao Huang and Andros Tjandra and Xiaohui Zhang and Frank Zhang and Christian Fuegen and Geoffrey Zweig and Michael L. Seltzer},
	title = {Transformer-Based Acoustic Modeling for Hybrid Speech Recognition},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})}
}

@inproceedings{jorge2021iberspeech,
  author={Javier Jorge and Adrià Giménez and Pau Baquero-Arnal and Javier Iranzo-Sánchez and Alejandro Pérez and Gonçal V. {Garcés Díaz-Munío} and Joan Albert Silvestre-Cerdà and Jorge Civera and Albert Sanchis and Alfons Juan},
  title={{MLLP-VRAIN Spanish ASR Systems for the Albayzin-RTVE 2020 Speech-To-Text Challenge}},
  year=2021,
  booktitle={Proc. IberSPEECH 2021},
  pages={118--122},
  doi={10.21437/IberSPEECH.2021-25}
}

@misc{wang2019transformers_lm,
  doi = {10.48550/ARXIV.1904.09408},
  url = {https://arxiv.org/abs/1904.09408},
  author = {Wang, Chenguang and Li, Mu and Smola, Alexander J.},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Language Models with Transformers},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{park2019specaugment,
	doi = {10.21437/interspeech.2019-2680},
	url = {https://doi.org/10.21437%2Finterspeech.2019-2680},
	year = 2019,
	month = {10},
	publisher = {ISCA},
	author = {Daniel S. Park and William Chan and Yu Zhang and Chung-Cheng Chiu and Barret Zoph and Ekin D. Cubuk and Quoc V. Le},
	title = {SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition},
	booktitle = {Interspeech 2019}
}

@ARTICLE{rumelhart1986backpropagation,
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  title = {Learning representations by back-propagating errors},
  journal = {Nature},
  year = 1986,
  month = oct,
  volume = {323},
  number = {6088},
  pages = {533-536},
  doi = {10.1038/323533a0},
  adsurl = {https://ui.adsabs.harvard.edu/abs/1986Natur.323..533R},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{HORNIK1989359,
  title = {Multilayer feedforward networks are universal approximators},
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359-366},
  year = {1989},
  issn = {0893-6080},
  doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
  url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
  author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
  keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}

@article{hochreiter1997lstm,
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  title = {Long Short-Term Memory},
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735-1780},
  year = {1997},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  url = {https://doi.org/10.1162/neco.1997.9.8.1735},
  eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}

@inproceedings{diazmunio21_interspeech,
  author={Gonçal V. Garcés Díaz-Munío and Joan-Albert Silvestre-Cerdà and Javier Jorge and Adrià Giménez Pastor and Javier Iranzo-Sánchez and Pau Baquero-Arnal and Nahuel Roselló and Alejandro Pérez-González-de-Martos and Jorge Civera and Albert Sanchis and Alfons Juan},
  title={{Europarl-ASR: A Large Corpus of Parliamentary Debates for Streaming ASR Benchmarking and Speech Data Filtering/Verbatimization}},
  year=2021,
  booktitle={Proc. Interspeech 2021},
  pages={3695--3699},
  doi={10.21437/Interspeech.2021-1905}
}

@misc{https://doi.org/10.48550/arxiv.2204.10586,
  doi = {10.48550/ARXIV.2204.10586},
  url = {https://arxiv.org/abs/2204.10586},
  author = {Zhou, Wei and Michel, Wilfried and Schlüter, Ralf and Ney, Hermann},
  keywords = {Computation and Language (cs.CL), Sound (cs.SD), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  title = {Efficient Training of Neural Transducer for Speech Recognition},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2203.12668,
  doi = {10.48550/ARXIV.2203.12668},
  url = {https://arxiv.org/abs/2203.12668},
  author = {Hwang, Dongseong and Sim, Khe Chai and Huo, Zhouyuan and Strohman, Trevor},
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Pseudo Label Is Better Than Human Label},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{10.2307/2984875,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2984875},
 abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
 author = {A. P. Dempster and N. M. Laird and D. B. Rubin},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {1--38},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Maximum Likelihood from Incomplete Data via the EM Algorithm},
 urldate = {2022-06-30},
 volume = {39},
 year = {1977}
}

@ARTICLE{hinton2012,
  author={Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E. and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N. and Kingsbury, Brian},
  journal={IEEE Signal Processing Magazine},
  title={Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups},
  year={2012},
  volume={29},
  number={6},
  pages={82-97}, 
  doi={10.1109/MSP.2012.2205597}
  }

@INPROCEEDINGS{mllp2020,
  author={Jorge, Javier and Giménez, Adrià and Iranzo-Sánchez, Javier and Silvestre-Cerdà, Joan Albert and Civera, Jorge and Sanchis, Albert and Juan, Alfons},
  booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title={LSTM-Based One-Pass Decoder for Low-Latency Streaming}, 
  year={2020},  
  volume={}, 
  number={}, 
  pages={7814-7818}, 
  doi={10.1109/ICASSP40776.2020.9054267}
}

@inproceedings{baqueroarnal20_interspeech,
  author={Pau Baquero-Arnal and Javier Jorge and Adrià Giménez and Joan Albert Silvestre-Cerdà and Javier Iranzo-Sánchez and Albert Sanchis and Jorge Civera and Alfons Juan},
  title={{Improved Hybrid Streaming ASR with Transformer Language Models}},
  year=2020,
  booktitle={Proc. Interspeech 2020},
  pages={2127--2131},
  doi={10.21437/Interspeech.2020-2770}
}

@inproceedings{jorge19_interspeech,
  author={Javier Jorge and Adrià Giménez and Javier Iranzo-Sánchez and Jorge Civera and Albert Sanchis and Alfons Juan},
  title={{Real-Time One-Pass Decoder for Speech Recognition Using LSTM Language Models}},
  year=2019,
  booktitle={Proc. Interspeech 2019},
  pages={3820--3824},
  doi={10.21437/Interspeech.2019-2798}
}

@article{BISANI2008434,
  title = {Joint-sequence models for grapheme-to-phoneme conversion},
  journal = {Speech Communication},
  volume = {50},
  number = {5},
  pages = {434-451},
  year = {2008},
  issn = {0167-6393},
  doi = {https://doi.org/10.1016/j.specom.2008.01.002},
  url = {https://www.sciencedirect.com/science/article/pii/S0167639308000046},
  author = {Maximilian Bisani and Hermann Ney},
  keywords = {Grapheme-to-phoneme, Letter-to-sound, Phonemic transcription, Joint-sequence model, Pronunciation modeling},
  abstract = {Grapheme-to-phoneme conversion is the task of finding the pronunciation of a word given its written form. It has important applications in text-to-speech and speech recognition. Joint-sequence models are a simple and theoretically stringent probabilistic framework that is applicable to this problem. This article provides a self-contained and detailed description of this method. We present a novel estimation algorithm and demonstrate high accuracy on a variety of databases. Moreover, we study the impact of the maximum approximation in training and transcription, the interaction of model size parameters, n-best list generation, confidence measures, and phoneme-to-grapheme conversion. Our software implementation of the method proposed in this work is available under an Open Source license.}
}

@inproceedings{amara,
  title = "The {AMARA} Corpus: Building Parallel Language Resources for the Educational Domain",
  author = "Abdelali, Ahmed  and
    Guzman, Francisco  and
    Sajjad, Hassan  and
    Vogel, Stephan",
  booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
  month = may,
  year = "2014",
  address = "Reykjavik, Iceland",
  publisher = "European Language Resources Association (ELRA)",
  url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/877_Paper.pdf",
  pages = "1856--1862",
}

@article{dgt,
  language = {eng},
  number = {4},
  pages = {679-707},
  publisher = {Springer},
  title = {An overview of the European Union's highly multilingual parallel corpora},
  volume = {48},
  year = {2014},
  author = {Steinberger, Ralf and Ebrahim, Mohamed and Poulis, Alexandros and Carrasco-Benitez, Manuel and Schlüter, Patrick and Przybyszewski, Marek and Gilbro, Signe},
  address = {Dordrecht},
  copyright = {Springer Science+Business Media 2014},
  issn = {1574-020X},
  journal = {Language Resources and Evaluation},
}

@inproceedings{europarl,
  added-at = {2010-11-10T15:46:05.000+0100},
  address = {Phuket, Thailand},
  author = {Koehn, Philipp},
  biburl = {https://www.bibsonomy.org/bibtex/205e5fa13fd7ba7992aedfe3514379a1f/unhammer},
  booktitle = {{Conference Proceedings: the tenth Machine Translation Summit}},
  interhash = {abd06b551527865eb50e21508841b6de},
  intrahash = {05e5fa13fd7ba7992aedfe3514379a1f},
  keywords = {Corpus Europarl MT Master},
  organization = {AAMT},
  pages = {79--86},
  publisher = {AAMT},
  timestamp = {2010-11-10T15:46:05.000+0100},
  title = {{Europarl: A Parallel Corpus for Statistical Machine Translation}},
  url = {http://mt-archive.info/MTS-2005-Koehn.pdf},
  year = 2005
}

@inproceedings{ncv8,
    title = "A Richly Annotated, Multilingual Parallel Corpus for Hybrid Machine Translation",
    author = "Avramidis, Eleftherios  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      van Genabith, Josef  and
      Melero, Maite  and
      Pecina, Pavel",
    booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)",
    month = may,
    year = "2012",
    address = "Istanbul, Turkey",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2012/pdf/444_Paper.pdf",
    pages = "2189--2193",
}

@inproceedings{opensubtitles-article,
  title={News from OPUS — A collection of multilingual parallel corpora with tools and interfaces},
  author={J{\"o}rg Tiedemann},
  year={2009}
}

@inproceedings{wit3,
  title = "{WIT}3: Web Inventory of Transcribed and Translated Talks",
  author = "Cettolo, Mauro  and
    Girardi, Christian  and
    Federico, Marcello",
  booktitle = "Proceedings of the 16th Annual conference of the European Association for Machine Translation",
  month = may # " 28{--}30",
  year = "2012",
  address = "Trento, Italy",
  publisher = "European Association for Machine Translation",
  url = "https://aclanthology.org/2012.eamt-1.60",
  pages = "261--268",
}

@article{JAIN2010651,
  title = {Data clustering: 50 years beyond K-means},
  journal = {Pattern Recognition Letters},
  volume = {31},
  number = {8},
  pages = {651-666},
  year = {2010},
  note = {Award winning papers from the 19th International Conference on Pattern Recognition (ICPR)},
  issn = {0167-8655},
  doi = {https://doi.org/10.1016/j.patrec.2009.09.011},
  url = {https://www.sciencedirect.com/science/article/pii/S0167865509002323},
  author = {Anil K. Jain},
  keywords = {Data clustering, User’s dilemma, Historical developments, Perspectives on clustering, King-Sun Fu prize},
  abstract = {Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in 1955. In spite of the fact that K-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, K-means is still widely used. This speaks to the difficulty in designing a general purpose clustering algorithm and the ill-posed problem of clustering. We provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection during data clustering, and large scale data clustering.}
}

@INPROCEEDINGS{8732232,
  author={Kathania, Hemant K. and Shahnawazuddin, Syed and Ahmad, Waquar and Adiga, Nagaraj},
  booktitle={2019 National Conference on Communications (NCC)}, 
  title={On the Role of Linear, Mel and Inverse-Mel Filterbank in the Context of Automatic Speech Recognition}, 
  year={2019},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/NCC.2019.8732232}
}

@ARTICLE{6857341,
  author={Abdel-Hamid, Ossama and Mohamed, Abdel-rahman and Jiang, Hui and Deng, Li and Penn, Gerald and Yu, Dong},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Convolutional Neural Networks for Speech Recognition}, 
  year={2014},
  volume={22},
  number={10},
  pages={1533-1545},
  doi={10.1109/TASLP.2014.2339736}
}

@InProceedings{tlk,
  author="del-Agua, M. A. and Gim{\'e}nez, A. and Serrano, N. and Andr{\'e}s-Ferrer, J. and Civera, J. and Sanchis, A. and Juan, A.",
  editor="Navarro Mesa, Juan Luis and Ortega, Alfonso and Teixeira, Ant{\'o}nio and Hern{\'a}ndez P{\'e}rez, Eduardo and Quintana Morales, Pedro and Ravelo Garc{\'i}a, Antonio and Guerra Moreno, Iv{\'a}n and Toledano, Doroteo T.",
  title="The Translectures-UPV Toolkit",
  booktitle="Advances in Speech and Language Technologies for Iberian Languages",
  year="2014",
  publisher="Springer International Publishing",
  address="Cham",
  pages="269--278",
  abstract="Over the past few years, online multimedia educational repositories have increased in number and popularity. The main aim of the transLectures project is to develop cost-effective solutions for producing accurate transcriptions and translations for large video lecture repositories, such as VideoLectures.NET or the Universitat Polit{\`e}cnica de Val{\`e}ncia's repository, poliMedia. In this paper, we present the transLectures-UPV toolkit (TLK), which has been specifically designed to meet the requirements of the transLectures project, but can also be used as a conventional ASR toolkit. The main features of the current release include HMM training and decoding with speaker adaptation techniques (fCMLLR). TLK has been tested on the VideoLectures.NET and poliMedia repositories, yielding very competitive results. TLK has been released under the permissive open source Apache License v2.0 and can be directly downloaded from the transLectures website.",
  isbn="978-3-319-13623-3"
}

@inproceedings{translectures,
    title = {transLectures},
    author = {Silvestre-Cerd\`a, Joan Albert and Del Agua, Miguel and Gon\c{c}al Garc\'es and Guillem Gasc\'o and Adri\`a Gim\'enez-Pastor and Adri\`a Mart\'inez and P\'erez Gonz\'alez de Martos, Alejandro and Isa\'ias S\'anchez and Mart\'inez-Santos, Nicol\'as Serrano and Rachel Spencer and Valor Mir\'o, Juan Daniel and Jes\'us Andr\'es-Ferrer and Jorge Civera and Alberto Sanch\'is and Alfons Juan},
    url = {http://hdl.handle.net/10251/37290
http://www.mllp.upv.es/wp-content/uploads/2015/04/1209IberSpeech.pdf},
    year = {2012},
    date = {2012-11-22},
    booktitle = {Proceedings of IberSPEECH 2012},
    pages = {345-351},
    address = {Madrid (Spain)},
    abstract = {transLectures (Transcription and Translation of Video Lectures) is an EU STREP project in which advanced automatic speech recognition and machine translation techniques are being tested on large video lecture repositories. The project began in November 2011 and will run for three years. This paper will outline the project\'s main motivation and objectives, and give a brief description of the two main repositories being considered: VideoLectures.NET and poliMedia. The first results obtained by the UPV group for the poliMedia repository will also be provided.},
    keywords = {Accessibility, Automatic Speech Recognition, Education, Intelligent Interaction, Language Technologies, Machine Translation, Massive Adaptation, Multilingualism, Opencast Matterhorn, Video Lectures}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%    Books
%

@book{jurafskySLP,
  title={Speech and Language Processing},
  edition={Third Edition draft},
  author={Jurafsky, Daniel and H. Martin, James},
  year={2021},
  publisher={Prentice-Hall},
  pages={167}
}

@book{pml1Book,
  title = "Probabilistic Machine Learning: An introduction",
  author = "Kevin P. Murphy",
  publisher = "MIT Press",
  year = 2022,
  url = "probml.ai"
}

@book{grenander1959probability,
  title={Probability and Statistics: The Harald Cram{\'e}r Volume},
  author={Grenander, U. and Cram{\'e}r, H. and Karreman Mathematics Research Collection},
  lccn={60000229},
  series={Wiley publications in statistics},
  year={1959},
  publisher={Almqvist \& Wiksell}
}

@book{mit97machinelearning,
title={Machine Learning},
author={Tom Mitchell},
publisher={McGraw Hill},
year={1997}
}

@book{russell2020artificial,
  title={Artificial Intelligence: A Modern Approach},
  edition={Fourth Edition},
  author={Russell, S. and Norvig, P.},
  year={2020},
  publisher={Pearson Education}
}

@book{Yu2015AutomaticSR,
  title={Automatic Speech Recognition: A Deep Learning Approach},
  author={Dong Yu and Li Deng},
  year={2015},
  publisher={Springer}
}

@book{bishop2006pattern,
  author = {Bishop, Christopher},
  title = {Pattern Recognition and Machine Learning},
  year = {2006},
  month = {1},
  publisher = {Springer},
  url = {https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/},
}

@book{mathews2012complex,
  title={Complex Analysis for Mathematics and Engineering},
  author={Mathews, J. and Howell, R.},
  isbn={9781449604455},
  lccn={2010043036},
  series={International series in mathematics},
  url={https://books.google.es/books?id=KaYuyybIqnMC},
  year={2012},
  publisher={Jones \& Bartlett Learning}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%    Web articles
%

@misc{bias_gender_discrim,
  title={Amazon scraps secret AI recruiting tool that showed bias against women},
  editor={Reuters},
  author={Dastin, Jeffrey},
  year={2018},
  url={https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G}
}

@misc{bias_racial_discrim,
  title={Racial bias in a medical algorithm favors white patients over sicker black patients},
  editor={The Washington Post},
  author={Y. Johnson, Carolyn},
  year={2019},
  url={https://www.washingtonpost.com/health/2019/10/24/racial-bias-medical-algorithm-favors-white-patients-over-sicker-black-patients/}
}

@misc{bias_sexual_discrim,
  title={A 'sexist' search bug says more about us than facebook},
  editor={Wired},
  author={Matsakis, Louise},
  year={2019},
  url={https://www.washingtonpost.com/health/2019/10/24/racial-bias-medical-algorithm-favors-white-patients-over-sicker-black-patients/}
}

@misc{colah2015lstm,
  title={Understanding LSTM Networks},
  author={Christopher Olah},
  year={2015},
  url={http://colah.github.io/posts/2015-08-Understanding-LSTMs/}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%    Altres
%

@misc{mllp,
  author={The Machine Learning and Language Processing research group (MLLP)},
  howpublished={\url{https://www.mllp.upv.es/}}
}

@misc{polimedia,
  author={Universitat Politècnica de València},
  howpublished={\url{https://www.upv.es/entidades/ASIC/catalogo/522359normalc.html}}
}

@misc{emma_project,
  author={Projecte de l'Unió Europea},
  title={European Multiple MOOC Aggregator},
  howpublished={\url{https://platform.europeanmoocs.eu/}}
}

@misc{voxforge,
  author={Voxforge},
  howpublished={\url{http://www.voxforge.org/}}
}

@misc{tensorflow,
  author={Google Brain Team},
  howpublished={\url{https://www.tensorflow.org/}}
}

@misc{cern,
  author={Organització Europea per a la Recerca Nuclear (CERN)},
  howpublished={https://home.cern/}
}

@misc{eu-tt2,
	title = {TransType2 European project.},
	url = {http://cordis.europa.eu/project/rcn/71419_en.html},
  howpublished = {\url{http://cordis.europa.eu/project/rcn/71419_en.html}}
}

@misc{eutv,
	title = {Multimedia Centre from the European Parliament.},
	url = {https://multimedia.europarl.europa.eu/en},
  howpublished = {\url{https://multimedia.europarl.europa.eu/en}}
}
@misc{opensubtitles,
	title = {Opensubtitles initiative.},
	url = {http://www.opensubtitles.org/},
  howpublished = {\url{http://www.opensubtitles.org/}}
}

@misc{wiki,
  author={Wikipédia L'encyclopédie libre},
  howpublished={\url{https://fr.wikipedia.org}}

}@misc{ted,
  author={},
  howpublished={\url{https://www.ted.com/}}
}

@misc{srilm,
  author={SRI Speech Technology and Research Laboratory},
  howpublished={http://www.speech.sri.com/projects/srilm/}
}

@misc{kenlm,
  author={Heafield, Kenneth},
  howpublished={https://kheafield.com/code/kenlm/}
}

@mics{fairseq,
  author={Facebook},
  howpublished={https://github.com/facebookresearch/fairseq}
}